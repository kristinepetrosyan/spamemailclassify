{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# importing dependencies here\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\n\n# preprocessing\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\n\nnltk.download(\"stopwords\")\n\n# lemmitizing\nfrom nltk.stem import WordNetLemmatizer\n\n# vectorization and pipeline\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import Normalizer\n\n# class imbalance\nfrom imblearn.pipeline import make_pipeline as imb_make_pipeline\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import RandomOverSampler\n\n# ML models and Cross Validation\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import svm\nfrom sklearn.model_selection import cross_validate\n\n# model evaluation\nfrom imblearn.metrics import classification_report_imbalanced\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.metrics import roc_auc_score\n\n# sparse to dense\nfrom sklearn.base import TransformerMixin\n\n\nclass DenseTransformer(TransformerMixin):\n    def fit(self, X, y=None, **fit_params):\n        return self\n\n    def transform(self, X, y=None, **fit_params):\n        return X.todense()\n\n\n# saving the model\nfrom joblib import dump\n\n\n# performance check\nimport time","execution_count":2,"outputs":[{"output_type":"stream","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reading the email data\ndf = pd.read_csv(os.path.join(\"..\", \"input\",\"email-spam-classification-dataset-csv\", \"emails.csv\"))","execution_count":7,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '../input/email-spam-classification-dataset-csvemails.csv'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-9a55ed2e77c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# reading the email data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"..\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"input\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"email-spam-classification-dataset-csv\"\u001b[0m \u001b[0;34m\"emails.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/email-spam-classification-dataset-csvemails.csv'"]}]},{"metadata":{"trusted":false},"cell_type":"code","source":"# checking first 5 records\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EDA"},{"metadata":{"trusted":false},"cell_type":"code","source":"# checking for class imbalance\ndf[\"spam\"].value_counts().plot(kind=\"bar\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Class 1 = Spam\n* Class 0 = Ham\n#### The dataset is imbalanced. I will handle class imbalance in the model pipeline."},{"metadata":{},"cell_type":"markdown","source":"### Checking for Null Values"},{"metadata":{"trusted":false},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Some quick Stats check"},{"metadata":{"trusted":false},"cell_type":"code","source":"print(df.info())\nprint(df.describe())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Cleaning"},{"metadata":{"trusted":false},"cell_type":"code","source":"# converting posts into lower case\ndf[\"clean_text\"] = df[\"text\"].str.lower()\n\ndf[\"clean_text\"] = df[\"clean_text\"].str.replace(\n    re.compile(r\"https?:\\/\\/(www)?.?([A-Za-z_0-9-]+)([\\S])*\"), \"\"\n)\n\n# dropping emails\ndf[\"clean_text\"] = df[\"clean_text\"].str.replace(re.compile(r\"\\S+@\\S+\"), \"\")\n\n# dropping punctuations\ndf[\"clean_text\"] = df[\"clean_text\"].str.replace(re.compile(r\"[^a-z\\s]\"), \" \")\n\n# dropping the word \"subject\"\ndf[\"clean_text\"] = df[\"clean_text\"].str.replace(\"subject\", \"\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lemmitizing"},{"metadata":{"trusted":false},"cell_type":"code","source":"# lemmitizing (excluding stop words in this step)\n\nt = time.time()\n\nlemmatizer = WordNetLemmatizer()\n\ndf[\"clean_text\"] = df[\"clean_text\"].apply(\n    lambda x: \" \".join(\n        [\n            lemmatizer.lemmatize(word)\n            for word in x.split(\" \")\n            if word not in stopwords.words(\"english\")\n        ]\n    )\n)\n\nprint(f\"Lemmitizing Time: {time.time() - t} seconds\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Building ML Model"},{"metadata":{"trusted":false},"cell_type":"code","source":"X = df[\"clean_text\"].values\ny = df[\"spam\"].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def build_model(model, X, y):\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42, stratify=y\n    )\n\n    # model training\n    model.fit(X_train, y_train)\n\n    # y_hat\n    y_pred = model.predict(X_test)\n\n    # model evaluation\n    print(classification_report_imbalanced(y_test, y_pred))\n\n    cross_validation_report(model, X, y)\n\n\n########################################################################################################\n\n\ndef build_proba_model(model, X, y):\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42, stratify=y\n    )\n\n    # model training\n    model.fit(X_train, y_train)\n\n    # y_hat\n    y_pred = model.predict(X_test)\n\n    # y_probability\n    y_proba = model.predict_proba(X_test)[:, 1]\n\n    # precision recall score\n    average_precision = average_precision_score(y_test, y_proba)\n\n    # model evaluation\n    print(f\"ROC-AUC Score: {roc_auc_score(y_test, y_proba):.2f}\")\n    print(f\"Average Precision-Recall Score: {average_precision:.2f}\")\n    print(classification_report_imbalanced(y_test, y_pred))\n\n    cross_validation_report(model, X, y)\n\n\n#####################################################################################################\n\n\ndef cross_validation_report(model, X, y):\n\n    raw_cv_report = cross_validate(\n        model, X, y, cv=5, scoring=(\"accuracy\", \"precision\", \"recall\")\n    )\n\n    cv_report = {f\"Avg {key}\": raw_cv_report[key].mean() for key in raw_cv_report}\n\n    print(\"Cross Validation Report:\\n--------------------------------\")\n    for key in cv_report:\n        print(f\"{key}: {cv_report[key]}\")\n\n    return","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Naive Bayes using Count Vectorizer"},{"metadata":{"trusted":false},"cell_type":"code","source":"ct_nb = imb_make_pipeline(\n    CountVectorizer(min_df=25, max_df=0.85, stop_words=\"english\"),\n    RandomOverSampler(),\n    MultinomialNB(class_prior=None, fit_prior=True),\n)\n\nbuild_proba_model(ct_nb, X, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Naive Bayes using TF-IDF Vectorizer"},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"tf_nb = imb_make_pipeline(\n    TfidfVectorizer(min_df=25, max_df=0.85, stop_words=\"english\"),\n    RandomOverSampler(),\n    MultinomialNB(class_prior=None, fit_prior=True),\n)\n\nbuild_proba_model(tf_nb, X, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Gaussian Naive Bayes using TF-IDF Vectorizer"},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"tf_gaussian_nb = imb_make_pipeline(\n    TfidfVectorizer(min_df=25, max_df=0.85, stop_words=\"english\"),\n    RandomOverSampler(),\n    DenseTransformer(),\n    GaussianNB(),\n)\n\nbuild_proba_model(tf_gaussian_nb, X, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## SVM with Count Vectorizer"},{"metadata":{},"cell_type":"markdown","source":"### RBF kernel with variations of C "},{"metadata":{"trusted":false},"cell_type":"code","source":"# C = 1 (default)\nsvm_pipe = imb_make_pipeline(\n    CountVectorizer(stop_words=\"english\"),\n    RandomOverSampler(),\n    svm.SVC(kernel=\"rbf\", C=1),\n)\nbuild_model(svm_pipe, X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# C = 10\nsvm_pipe = imb_make_pipeline(\n    CountVectorizer(stop_words=\"english\"),\n    RandomOverSampler(),\n    svm.SVC(kernel=\"rbf\", C=10),\n)\nbuild_model(svm_pipe, X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# C = 100\nsvm_pipe = imb_make_pipeline(\n    CountVectorizer(stop_words=\"english\"),\n    RandomOverSampler(),\n    svm.SVC(kernel=\"rbf\", C=100),\n)\nbuild_model(svm_pipe, X, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Linear kernel with variations of C "},{"metadata":{"trusted":false},"cell_type":"code","source":"# C = 1 (default)\nsvm_pipe = imb_make_pipeline(\n    CountVectorizer(stop_words=\"english\"),\n    RandomOverSampler(),\n    svm.SVC(kernel=\"linear\", C=1),\n)\nbuild_model(svm_pipe, X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# C = 10\nsvm_pipe = imb_make_pipeline(\n    CountVectorizer(stop_words=\"english\"),\n    RandomOverSampler(),\n    svm.SVC(kernel=\"linear\", C=10),\n)\nbuild_model(svm_pipe, X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# C = 100\nsvm_pipe = imb_make_pipeline(\n    CountVectorizer(stop_words=\"english\"),\n    RandomOverSampler(),\n    svm.SVC(kernel=\"linear\", C=100),\n)\nbuild_model(svm_pipe, X, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Poly kernel with variations of C"},{"metadata":{"trusted":false},"cell_type":"code","source":"# C = 1, default degree = 3\nsvm_pipe = imb_make_pipeline(\n    CountVectorizer(stop_words=\"english\"),\n    RandomOverSampler(),\n    svm.SVC(kernel=\"poly\", C=1, degree=3),\n)\nbuild_model(svm_pipe, X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# C = 10\nsvm_pipe = imb_make_pipeline(\n    CountVectorizer(stop_words=\"english\"),\n    RandomOverSampler(),\n    svm.SVC(kernel=\"poly\", C=10, degree=3),\n)\nbuild_model(svm_pipe, X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# C = 100\nsvm_pipe = imb_make_pipeline(\n    CountVectorizer(stop_words=\"english\"),\n    RandomOverSampler(),\n    svm.SVC(kernel=\"poly\", C=100, degree=3),\n)\nbuild_model(svm_pipe, X, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Based on the best scores for accuracy, precision and recall, selecting SVM with rbf kernel and regularization value of 10 as our final model."},{"metadata":{},"cell_type":"markdown","source":"#### Before training the final model on entire dataset, testing its performance with the variations of gamma parameter."},{"metadata":{"trusted":false},"cell_type":"code","source":"# gamma set to the default value of \"scale\"\nsvm_pipe = imb_make_pipeline(\n    CountVectorizer(stop_words=\"english\"),\n    RandomOverSampler(),\n    svm.SVC(kernel=\"rbf\", C=10, gamma=\"scale\"),\n)\nbuild_model(svm_pipe, X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# gamma set to \"auto\"\nsvm_pipe = imb_make_pipeline(\n    CountVectorizer(stop_words=\"english\"),\n    RandomOverSampler(),\n    svm.SVC(kernel=\"rbf\", C=10, gamma=\"auto\"),\n)\nbuild_model(svm_pipe, X, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Going ahead with the SVM trained model using default gamma value of scale and training it on the entire dataset"},{"metadata":{"trusted":false},"cell_type":"code","source":"# training the final model\nsvm_pipe_final = imb_make_pipeline(\n    CountVectorizer(stop_words=\"english\"),\n    RandomOverSampler(),\n    svm.SVC(kernel=\"rbf\", C=10, gamma=\"scale\"),\n)\nsvm_pipe_final.fit(X, y)\n\n# saving the model\ndump(svm_pipe_final, os.path.join(\"..\", \"model\", \"svm_spam_classifier.joblib\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"project3","language":"python","name":"project3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}